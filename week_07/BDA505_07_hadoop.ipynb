{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A TOUR OF HADOOP ECOSYSTEM THROUGH HORTONWORKS SANDBOX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hortonworks' \"Sandbox\" and Cloudera's \"Clusterdock\" are two self-contained docker images, that can run a full hadoop distribution in the stand-alone or semidistributed mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of them are installed in Lab 409 PC's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However for ease of use, its open-source nature, the variety of Hadoop services available out of the box and focus on teaching Hadoop, we will prefer Hortonworks Sandbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hortonworks Sandbox features a standalone version of the full \"Hortonworks Data Platform\" (HDP) Hadoop distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_sandbox-hdp.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will take several minutes to make the sandbox up and running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can communicate with the Sandbox through localhost:8888"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 12542\n"
     ]
    }
   ],
   "source": [
    "firefox localhost:8888 &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you hit the \"Launch Dashboard\" button, the login screen for Ambari will appear.\n",
    "\n",
    "Ambari is a completely open source management platform for provisioning, managing, monitoring and securing Apache Hadoop clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The credentials are: admin, bda505"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the ambari dashboard, you can access:\n",
    "- HDFS\n",
    "- YARN\n",
    "- MapReduce2\n",
    "- Hive\n",
    "- Hbase\n",
    "- Pig\n",
    "- Sqoop\n",
    "- Spark2\n",
    "- Zeppelin\n",
    "\n",
    "and other projects within the Hadoop ecosystem.\n",
    "We will see the use cases of all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pop-up window features a tutorial series for using HDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can follow the link to watch a video about Introduction to Sandbox:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Hadoop Tutorial: Introduction to Hortonworks Sandbox](https://www.youtube.com/watch?v=H0KXnfE9Z9s&list=PL2y_WpKCCNQcABNHVSwUwwK169xxwSdNp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can connect to our docker container using ssh.\n",
    "\n",
    "For this please open a shell screen and enter the following command (without the leading #). The ssh password of root is set as \"hadoopbda505\"\n",
    "\n",
    "Hopefully, passwordless ssh login is enabled as of this class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ssh root@localhost -p 2223"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we show a simple data import and map reduce job:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Data import and Map Reduce Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to import data into sandbox, we have to send the data to the docker container through ssh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data are sent to the docker cotainer from the main system through scp or rsync commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 datasets are pushed to the docker container:\n",
    "\n",
    "- The imdb dataset as tsv files\n",
    "- A portion of the 2015 UN COMTRADE dataset (international trade statistics) as json files \n",
    "- 1901-1950 portion of the ncdc dataset (US weather statistics) as plain text files\n",
    "- A portion of the google ngrams dataset (frequency of words in publications indexed by Google Books, by year of publication) as plain text files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bash commands to create the necessary directories inside the docker container are as follows\n",
    "\n",
    "Note that, this command is executed from the main system, not from inside the docker. It is executed via ssh:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data directories\n",
    "#ssh root@localhost -p 2223 'yum -y install expect;\n",
    "#if [ ! -d /data ]; then mkdir /data; fi;\n",
    "#if [ ! -d /data/imdb ]; then mkdir /data/imdb; fi;\n",
    "#if [ ! -d /data/comtrade ]; then mkdir /data/comtrade; fi;\n",
    "#if [ ! -d /data/comtrade/gz ]; then mkdir /data/comtrade/gz; fi;\n",
    "#if [ ! -d /data/ncdc ]; then mkdir /data/ncdc; fi;\n",
    "#if [ ! -d /data/ncdc/gz ]; then mkdir /data/ncdc/gz; fi;\n",
    "#if [ ! -d /data/ngrams ]; then mkdir /data/ngrams; fi;\n",
    "#if [ ! -d /data/ngrams/gz ]; then mkdir /data/ngrams/gz; fi;'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we push the data into the container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rsync -e 'ssh -p 2223' -a /home/bda505/mef/01/Session_01_dataset/tsv/*2.tsv root@localhost:/data/imdb/;\n",
    "#rsync -e 'ssh -p 2223' -a /home/bda505/mef/04/comtrade_2015/gz/2015_{1..152}_* root@localhost:/data/comtrade/gz;\n",
    "#rsync -e 'ssh -p 2223' -a /home/bda505/mef/06/ncdc/{1901..1950}* root@localhost:/data/ncdc/gz;\n",
    "#rsync -e 'ssh -p 2223' -a /home/bda505/mef/07/ngrams/googlebooks-eng-all-1gram-20120701-*[0-9a].gz root@localhost:/data/ngrams/gz;\n",
    "\n",
    "#ssh root@localhost -p 2223 'if [ ! -d /data/comtrade/json ]; then cp -r /data/comtrade/gz /data/comtrade/json;\\\n",
    "#cd /data/comtrade/json; gunzip *.gz; fi;\n",
    "\n",
    "#if [ ! -d /data/ncdc/txt ]; then cp -r /data/ncdc/gz /data/ncdc/txt;\\\n",
    "#cd /data/ncdc/txt; gunzip *.gz; fi;\n",
    "\n",
    "#if [ ! -d /data/ngrams/txt ]; then cp -r /data/ngrams/gz /data/ngrams/txt;\\\n",
    "#cd /data/ngrams/txt; gunzip *.gz; fi\n",
    "\n",
    "#chmod -R 777 /data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the codes ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rsync -e 'ssh -p 2223' -a /home/bda505/mef-bigdata/week_07/codes root@localhost:/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over network connections, and a large count of files that my change in time, rsync is a better choice. It is small, fast yet versatile: Supports many different options.\n",
    "\n",
    "Rsync alone can be a perfect tool for backing up and synchronization even over remote connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a smaller number of files, scp may also be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scp -P 2223 /path/to/localfile root@localhost:~/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a little task for you:\n",
    "- First open a shell window\n",
    "- And open a second shell window\n",
    "- With \"ssh root@localhost -p 2223\" command, login to the docker container, so that you can view what is inside\n",
    "- In your \"real\" shell window - not the docker shell - download the 2014 flight data for NYC airports via:\n",
    "\"wget https://raw.githubusercontent.com/wiki/arunsrinivasan/flights/NYCflights14/flights14.csv\"\n",
    "- \"wget\" is also a very light, powerful and versatile command: You can even use it as a spider to download and create a mirror of a complete website!\n",
    "- Now push the csv file into the docker container and confirm it is inside the docker container (using scp or rsync)\n",
    "- You may create a separate directory inside the docker container\n",
    "- You may use \"ls\" command to list directory contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also access the shell inside the docker using the  shell web client or shell-in-a-box at localhost:4200\n",
    "\n",
    "Use the credentials: root, hadoopbda505"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may use shell commands for some tasks, and ambari web ui for other purposes\n",
    "\n",
    "The login details for ambari ui at localhost:8888 is admin, bda505\n",
    "\n",
    "If you cannot login with this password, use the below command from inside the docker, to input your new password:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ambari-admin-password-reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can upload data into HDFS either using ambari web ui or command line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's first see what our HDFS directories look like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From localhost:8888, click on dashboard, and login with admin, bda505"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to the left of the top right corner, select \"Files View\"\n",
    "\n",
    "That shows the contents of the root directory of HDFS\n",
    "\n",
    "You can create folders or upload files from your \"main\" system - that's the easy way\n",
    "\n",
    "But for the time being, we will upload data using command line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside docker shell or shell web client, change the user to hdfs, so that we can access the \"hdfs\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# su hdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And cd to root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd /"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the data directory that we pushed to the docker container with ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hdfs commands are prefixed with \"hdfs dfs\" followed by a dash and ordinary system commands of Linux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's enlarge the permissions of the user directory inside the root of hdfs so that any user can read, write and execute inside user directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdfs dfs -chmod 777 /user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, 777 is the most relaxed permission set and in production environments, it may be harmful: Think before you do it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add a directory to hdfs as /user/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdfs dfs -mkdir /user/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check that the directory is created checking the ambari UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, let's \"put\" our data inside docker container into the hdfs, let's do it for the ncdc data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdfs dfs -mkdir /user/data/ncdc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdfs dfs -put /data/ncdc/txt /user/data/ncdc/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can view the files are uploaded through Ambari UI or a command from docker shell: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdfs dfs -ls /data/ncdc/txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples to other commands that can be used with \"hdfs dfs\" are rm, cp, du and get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can have more information on hdfs commands at:\n",
    "https://hortonworks.com/hadoop-tutorial/using-commandline-manage-files-hdfs/\n",
    "\n",
    "and using HDFS from Ambari at:\n",
    "\n",
    "https://hortonworks.com/tutorial/hadoop-tutorial-getting-started-with-hdp/section/2/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import other datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdfs dfs -mkdir /user/data/imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdfs dfs -put /data/imdb/ /user/data/imdb/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdfs dfs -mkdir /user/data/ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdfs dfs -put /data/ngrams/txt /user/data/ngrams/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the code ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdfs dfs -put /data/codes /user/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, all configuration parameters - like the replication factor or the hostnames of the slaves, or the hostname of the master node -  are set inside a few simple xml files such as:\n",
    "\n",
    "- core-site.xml\n",
    "- hadoop-env.sh\n",
    "- hdfs-site.xml\n",
    "- mapred-site.xml\n",
    "- slaves\n",
    "\n",
    "under $HADOOP_HOME/etc/hadoop\n",
    "\n",
    "$HADOOP_HOME path may differ among systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Reduce Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the maximum temperature example from the Elephant Book as a \"Hadoop streaming\" job using:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your computers have a century of the temperature data of USA. We will use a portion of this data\n",
    "\n",
    "And for each year we will get the max temperature\n",
    "\n",
    "We will first start with standard unix tools\n",
    "\n",
    "I played a bit with the original script given as a part of the supplementary material for the Elephant Book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "#!/usr/bin/env bash\n",
    "\n",
    "# adjusted by Serhat Cevikel to measure the time\n",
    "\n",
    "#START=$(date +%s.%N)\n",
    "path=$1\n",
    "starty=$2\n",
    "endy=$3\n",
    "\n",
    "years=$(seq $starty $endy)\n",
    "\n",
    "\n",
    "for year in $years\n",
    "do\n",
    "    filee=\"${path}/${year}\"\n",
    "  echo -ne `basename $year .gz`\"\\t\"\n",
    "  gunzip -c $filee | \\ \n",
    "    awk '{ temp = substr($0, 88, 5) + 0;\n",
    "           q = substr($0, 93, 1);\n",
    "           if (temp !=9999 && q ~ /[01459]/ && temp > max) max = temp }\n",
    "         END { print max }'\n",
    "done\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing temp values are coded as 9999 and they are excluded\n",
    "\"q\" is a quality code and should be one of 0,1,4,5,9 to be included"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The path to script is /home/bda505/mef-bigdata/week_07/codes/hadoop_max_temperature.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my implementation it takes 3 parameters: the path to gz files, start year and end year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data resides at /home/bda505/mef/06/ncdc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do it for 1901 to 1925"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1901\t317\n",
      "1902\t244\n",
      "1903\t289\n",
      "1904\t256\n",
      "1905\t283\n",
      "1906\t294\n",
      "1907\t283\n",
      "1908\t289\n",
      "1909\t278\n",
      "1910\t\n",
      "1911\t306\n",
      "1912\t322\n",
      "1913\t300\n",
      "1914\t333\n",
      "1915\t294\n",
      "1916\t278\n",
      "1917\t317\n",
      "1918\t322\n",
      "1919\t378\n",
      "1920\t294\n",
      "1921\t283\n",
      "1922\t278\n",
      "1923\t\n",
      "1924\t\n",
      "1925\t317\n"
     ]
    }
   ],
   "source": [
    "/home/bda505/mef-bigdata/week_07/codes/hadoop_max_temperature.sh \\\n",
    "/home/bda505/mef/06/ncdc \\\n",
    "1901 1925"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's transform it to a map reduce job "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the map phase, the key value pairs are extracted from the data: the year and temp reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the reduce phase the max temp for each year is calculated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```R\n",
    "#!/usr/bin/Rscript\n",
    "\n",
    "liness <- readLines(file(\"stdin\"))\n",
    "#liness <- readLines(\"1901\")\n",
    "\n",
    "year <- as.numeric(substr(liness, 16, 19))\n",
    "temp <- as.numeric(substr(liness, 88, 92))\n",
    "qq <- as.numeric(substr(liness, 93, 93))\n",
    "\n",
    "output <- cbind(year, temp)\n",
    "\n",
    "output <- output[temp != 9999 & qq %in% c(0, 1, 4, 5, 9),]\n",
    "\n",
    "for (i in 1:nrow(output))\n",
    "{\n",
    "    pasted <- paste(output[i,], collapse = \" \")\n",
    "    cat(sprintf(\"%s\\n\", pasted))\n",
    "}\n",
    "```                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code accepts input from stdin so can be used similar to the previous one - before Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The path to code is /home/bda505/mef-bigdata/week_07/codes/mapper.R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1901 -78\n",
      "1901 -72\n",
      "1901 -94\n",
      "1901 -61\n",
      "1901 -56\n",
      "1901 -28\n",
      "1901 -67\n",
      "1901 -33\n",
      "1901 -28\n",
      "1901 -33\n",
      "Error in cat(sprintf(\"%s\\n\", pasted)) : ignoring SIGPIPE signal\n",
      "In addition: Warning message:\n",
      "In parent.env(env) : closing unused connection 3 (stdin)\n",
      "Execution halted\n"
     ]
    }
   ],
   "source": [
    "zcat /home/bda505/mef/06/ncdc/{1901..1925} | /home/bda505/mef-bigdata/week_07/codes/mapper.R | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reducer code is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```R\n",
    "#!/usr/bin/Rscript\n",
    "\n",
    "#liness <- readLines(file(\"stdin\"))\n",
    "liness <- readLines(\"mapped\")\n",
    "\n",
    "keyval <- list()\n",
    "\n",
    "for (i in 1:length(liness))\n",
    "{\n",
    "    linex <- unlist(strsplit(liness[i], split = \" \"))\n",
    "    key <- linex[1]\n",
    "    val <- as.numeric(linex[2])\n",
    "\n",
    "    cur.maxval <- keyval[[key]]\n",
    "\n",
    "    if (is.null(cur.maxval))\n",
    "    {\n",
    "        keyval[[key]] <- val\n",
    "    }\n",
    "    else\n",
    "    {\n",
    "        if (val > cur.maxval)\n",
    "        {\n",
    "            keyval[[key]] <- val\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "keys <- as.numeric(names(keyval))\n",
    "vals <- as.numeric(unlist(keyval))\n",
    "\n",
    "output <- matrix(c(keys, vals), ncol = 2)\n",
    "output <- output[order(keys),, drop = F]\n",
    "\n",
    "\n",
    "for (i in 1:nrow(output))\n",
    "{\n",
    "    pasted <- paste(output[i,], collapse = \" \")\n",
    "    cat(sprintf(\"%s\\n\", pasted))\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The path to the reducer code is /home/bda505/mef-bigdata/week_07/codes/reducer.R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "In parent.env(env) : closing unused connection 3 (stdin)\n",
      "Warning message:\n",
      "closing unused connection 3 (stdin) \n",
      "1901 317\n",
      "1902 244\n",
      "1903 289\n",
      "1904 256\n",
      "1905 283\n",
      "1906 294\n",
      "1907 283\n",
      "1908 289\n",
      "1909 278\n",
      "1911 306\n",
      "1912 322\n",
      "1913 300\n",
      "1914 333\n",
      "1915 294\n",
      "1916 278\n",
      "1917 317\n",
      "1918 322\n",
      "1919 378\n",
      "1920 294\n",
      "1921 283\n",
      "1922 278\n",
      "1925 317\n"
     ]
    }
   ],
   "source": [
    "zcat /home/bda505/mef/06/ncdc/{1901..1925} | \\\n",
    "/home/bda505/mef-bigdata/week_07/codes/mapper.R | \\\n",
    "/home/bda505/mef-bigdata/week_07/codes/reducer\n",
    ".R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the map reduce job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```R\n",
    "hadoop jar /usr/hdp/2.6.3.0-235/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-input /user/data/ncdc/txt \\\n",
    "-output /user/data/ncdc/output \\\n",
    "-mapper /data/codes/mapper.R \\\n",
    "-reducer /data/codes/reducer.R\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "248px",
    "left": "36px",
    "right": "1229px",
    "top": "110px",
    "width": "175px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
