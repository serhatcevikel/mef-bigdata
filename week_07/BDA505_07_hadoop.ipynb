{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#A-TOUR-OF-HADOOP-ECOSYSTEM-THROUGH-HORTONWORKS-SANDBOX\" data-toc-modified-id=\"A-TOUR-OF-HADOOP-ECOSYSTEM-THROUGH-HORTONWORKS-SANDBOX-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>A TOUR OF HADOOP ECOSYSTEM THROUGH HORTONWORKS SANDBOX</a></span><ul class=\"toc-item\"><li><span><a href=\"#Simple-Data-import-and-Map-Reduce-Job\" data-toc-modified-id=\"Simple-Data-import-and-Map-Reduce-Job-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Simple Data import and Map Reduce Job</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-Import\" data-toc-modified-id=\"Data-Import-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Data Import</a></span></li><li><span><a href=\"#Map-Reduce-Job\" data-toc-modified-id=\"Map-Reduce-Job-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Map Reduce Job</a></span><ul class=\"toc-item\"><li><span><a href=\"#Word-Count-on-NCDC-Dataset\" data-toc-modified-id=\"Word-Count-on-NCDC-Dataset-1.1.2.1\"><span class=\"toc-item-num\">1.1.2.1&nbsp;&nbsp;</span>Word Count on NCDC Dataset</a></span></li><li><span><a href=\"#Title-Count-Across-Years-Using-IMDB-Dataset\" data-toc-modified-id=\"Title-Count-Across-Years-Using-IMDB-Dataset-1.1.2.2\"><span class=\"toc-item-num\">1.1.2.2&nbsp;&nbsp;</span>Title Count Across Years Using IMDB Dataset</a></span></li></ul></li></ul></li><li><span><a href=\"#Creating-Scripts-for-MapReduce-Job\" data-toc-modified-id=\"Creating-Scripts-for-MapReduce-Job-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Creating Scripts for MapReduce Job</a></span><ul class=\"toc-item\"><li><span><a href=\"#Maximum-Temperature-Example-on-NCDC-Dataset\" data-toc-modified-id=\"Maximum-Temperature-Example-on-NCDC-Dataset-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>Maximum Temperature Example on NCDC Dataset</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A TOUR OF HADOOP ECOSYSTEM THROUGH HORTONWORKS SANDBOX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hortonworks' \"Sandbox\" and Cloudera's \"Clusterdock\" are two self-contained docker images, that can run a full hadoop distribution in the stand-alone or semidistributed mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of them are installed in Lab 409 PC's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However for ease of use, its open-source nature, the variety of Hadoop services available out of the box and focus on teaching Hadoop, we will prefer Hortonworks Sandbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hortonworks Sandbox features a standalone version of the full \"Hortonworks Data Platform\" (HDP) Hadoop distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start_sandbox-hdp.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will take several minutes to make the sandbox up and running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can communicate with the Sandbox through localhost:8888"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 12542\n"
     ]
    }
   ],
   "source": [
    "firefox localhost:8888 &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you hit the \"Launch Dashboard\" button, the login screen for Ambari will appear.\n",
    "\n",
    "Ambari is a completely open source management platform for provisioning, managing, monitoring and securing Apache Hadoop clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The credentials are: admin, bda505"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the ambari dashboard, you can access:\n",
    "- HDFS\n",
    "- YARN\n",
    "- MapReduce2\n",
    "- Hive\n",
    "- Hbase\n",
    "- Pig\n",
    "- Sqoop\n",
    "- Spark2\n",
    "- Zeppelin\n",
    "\n",
    "and other projects within the Hadoop ecosystem.\n",
    "We will see the use cases of all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pop-up window features a tutorial series for using HDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can follow the link to watch a video about Introduction to Sandbox:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Hadoop Tutorial: Introduction to Hortonworks Sandbox](https://www.youtube.com/watch?v=H0KXnfE9Z9s&list=PL2y_WpKCCNQcABNHVSwUwwK169xxwSdNp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can connect to our docker container using ssh.\n",
    "\n",
    "For this please open a shell screen and enter the following command (without the leading #). The ssh password of root is set as \"hadoopbda505\"\n",
    "\n",
    "Hopefully, passwordless ssh login is enabled as of this class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ssh root@localhost -p 2223"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we show a simple data import and map reduce job:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Data import and Map Reduce Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to import data into sandbox, we have to send the data to the docker container through ssh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data are sent to the docker cotainer from the main system through scp or rsync commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 datasets are pushed to the docker container:\n",
    "\n",
    "- The imdb dataset as tsv files\n",
    "- A portion of the 2015 UN COMTRADE dataset (international trade statistics) as json files \n",
    "- 1901-1950 portion of the ncdc dataset (US weather statistics) as plain text files\n",
    "- A portion of the google ngrams dataset (frequency of words in publications indexed by Google Books, by year of publication) as plain text files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bash commands to create the necessary directories inside the docker container are as follows\n",
    "\n",
    "Note that, this command is executed from the main system, not from inside the docker. It is executed via ssh:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data directories\n",
    "#ssh root@localhost -p 2223 'yum -y install expect;\n",
    "#if [ ! -d /data ]; then mkdir /data; fi;\n",
    "#if [ ! -d /data/imdb ]; then mkdir /data/imdb; fi;\n",
    "#if [ ! -d /data/comtrade ]; then mkdir /data/comtrade; fi;\n",
    "#if [ ! -d /data/comtrade/gz ]; then mkdir /data/comtrade/gz; fi;\n",
    "#if [ ! -d /data/ncdc ]; then mkdir /data/ncdc; fi;\n",
    "#if [ ! -d /data/ncdc/gz ]; then mkdir /data/ncdc/gz; fi;\n",
    "#if [ ! -d /data/ngrams ]; then mkdir /data/ngrams; fi;\n",
    "#if [ ! -d /data/ngrams/gz ]; then mkdir /data/ngrams/gz; fi;'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we push the data into the container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rsync -e 'ssh -p 2223' -a /home/bda505/mef/01/Session_01_dataset/tsv/*2.tsv root@localhost:/data/imdb/;\n",
    "#rsync -e 'ssh -p 2223' -a /home/bda505/mef/04/comtrade_2015/gz/2015_{1..152}_* root@localhost:/data/comtrade/gz;\n",
    "#rsync -e 'ssh -p 2223' -a /home/bda505/mef/06/ncdc/{1901..1950}* root@localhost:/data/ncdc/gz;\n",
    "#rsync -e 'ssh -p 2223' -a /home/bda505/mef/07/ngrams/googlebooks-eng-all-1gram-20120701-*[0-9a].gz root@localhost:/data/ngrams/gz;\n",
    "\n",
    "#ssh root@localhost -p 2223 'if [ ! -d /data/comtrade/json ]; then cp -r /data/comtrade/gz /data/comtrade/json;\\\n",
    "#cd /data/comtrade/json; gunzip *.gz; fi;\n",
    "\n",
    "#if [ ! -d /data/ncdc/txt ]; then cp -r /data/ncdc/gz /data/ncdc/txt;\\\n",
    "#cd /data/ncdc/txt; gunzip *.gz; fi;\n",
    "\n",
    "#if [ ! -d /data/ngrams/txt ]; then cp -r /data/ngrams/gz /data/ngrams/txt;\\\n",
    "#cd /data/ngrams/txt; gunzip *.gz; fi\n",
    "\n",
    "#chmod -R 777 /data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the codes ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rsync -e 'ssh -p 2223' -a /home/bda505/mef-bigdata/week_07/codes root@localhost:/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over network connections, and a large count of files that my change in time, rsync is a better choice. It is small, fast yet versatile: Supports many different options.\n",
    "\n",
    "Rsync alone can be a perfect tool for backing up and synchronization even over remote connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a smaller number of files, scp may also be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scp -P 2223 /path/to/localfile root@localhost:~/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a little task for you:\n",
    "- First open a shell window\n",
    "- And open a second shell window\n",
    "- With \"ssh root@localhost -p 2223\" command, login to the docker container, so that you can view what is inside\n",
    "- In your \"real\" shell window - not the docker shell - download the 2014 flight data for NYC airports via:\n",
    "\"wget https://raw.githubusercontent.com/wiki/arunsrinivasan/flights/NYCflights14/flights14.csv\"\n",
    "- \"wget\" is also a very light, powerful and versatile command: You can even use it as a spider to download and create a mirror of a complete website!\n",
    "- Now push the csv file into the docker container and confirm it is inside the docker container (using scp or rsync)\n",
    "- You may create a separate directory inside the docker container\n",
    "- You may use \"ls\" command to list directory contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also access the shell inside the docker using the  shell web client or shell-in-a-box at localhost:4200\n",
    "\n",
    "Use the credentials: root, hadoopbda505"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may use shell commands for some tasks, and ambari web ui for other purposes\n",
    "\n",
    "The login details for ambari ui at localhost:8888 is admin, bda505\n",
    "\n",
    "If you cannot login with this password, use the below command from inside the docker, to input your new password:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ambari-admin-password-reset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can upload data into HDFS either using ambari web ui or command line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's first see what our HDFS directories look like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From localhost:8888, click on dashboard, and login with admin, bda505"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to the left of the top right corner, select \"Files View\"\n",
    "\n",
    "That shows the contents of the root directory of HDFS\n",
    "\n",
    "You can create folders or upload files from your \"main\" system - that's the easy way\n",
    "\n",
    "But for the time being, we will upload data using command line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside docker shell or shell web client, change the user to hdfs, so that we can access the \"hdfs\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# su hdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And cd to root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd /"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the data directory that we pushed to the docker container with ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hdfs commands are prefixed with \"hdfs dfs\" followed by a dash and ordinary system commands of Linux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's enlarge the permissions of the user directory inside the root of hdfs so that any user can read, write and execute inside user directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdfs dfs -chmod 777 /user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, 777 is the most relaxed permission set and in production environments, it may be harmful: Think before you do it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add a directory to hdfs as /user/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdfs dfs -mkdir /user/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check that the directory is created checking the ambari UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, let's \"put\" our data inside docker container into the hdfs, let's do it for the ncdc data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdfs dfs -mkdir /user/data/ncdc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdfs dfs -put /data/ncdc/txt /user/data/ncdc/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can view the files are uploaded through Ambari UI or a command from docker shell: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdfs dfs -ls /data/ncdc/txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples to other commands that can be used with \"hdfs dfs\" are rm, cp, du and get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can have more information on hdfs commands at:\n",
    "https://hortonworks.com/hadoop-tutorial/using-commandline-manage-files-hdfs/\n",
    "\n",
    "and using HDFS from Ambari at:\n",
    "\n",
    "https://hortonworks.com/tutorial/hadoop-tutorial-getting-started-with-hdp/section/2/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import other datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdfs dfs -mkdir /user/data/imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdfs dfs -put /data/imdb/ /user/data/imdb/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdfs dfs -mkdir /user/data/ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdfs dfs -put /data/ngrams/txt /user/data/ngrams/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the code ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdfs dfs -put /data/codes /user/data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, all configuration parameters - like the replication factor or the hostnames of the slaves, or the hostname of the master node -  are set inside a few simple xml files such as:\n",
    "\n",
    "- core-site.xml\n",
    "- hadoop-env.sh\n",
    "- hdfs-site.xml\n",
    "- mapred-site.xml\n",
    "- slaves\n",
    "\n",
    "under $HADOOP_HOME/etc/hadoop\n",
    "\n",
    "$HADOOP_HOME path may differ among systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Reduce Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will go through two examples using the first 10 years of the ncdc weather data set:\n",
    "\n",
    "- Word Count\n",
    "- Max Temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should keep open a shell windows inside the sandbox-hdp\n",
    "\n",
    "Either\n",
    "\n",
    "`ssh -p 2223 root@localhost:8888`\n",
    "\n",
    "or\n",
    "\n",
    "on the browser navigate to:\n",
    "\n",
    "`localhost:4200`\n",
    "\n",
    "The root password is set as hadoopbda505 (if it doesn't work, try \"hadoop\")\n",
    "\n",
    "After login, change hdfs user by:\n",
    "\n",
    "`su hdfs`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Count on NCDC Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is the \"hello world\" of map reduce and also cited inside the official documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will get the total line count of all 10 files in the dataset\n",
    "\n",
    "We will first implement the map reduce job as a unix command with pipes\n",
    "\n",
    "If this works right, we will run it using hadoop streaming\n",
    "\n",
    "Two versions will be run:\n",
    "- One with a mapper and reducer,\n",
    "- And the other with a mapper, combiner and reducer\n",
    "\n",
    "In larger datasets run on many nodes, it is good practice to use a combiner so that the network traffic between the mapper and reducer is minimized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that all codes/commands must accept data from stdin and emit the result to stdout\n",
    "It is best to delimit fields by tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mapper phase creates a key value pair out of the data. In the wc example, the original files are send to stdout \n",
    "\n",
    "\"head\" command exists just in order to limit the visible output and will not be a part of the mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat /data/ncdc/txt/* | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the reducer will count the lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat /data/ncdc/txt/* | wc -l\n",
    "# 57920"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will convert this into a simple map reduce job:\n",
    "\n",
    "- Note that the output folder should not exist, so flush it before running the job\n",
    "- If the command takes parameters, wrap it around single or double quotes\n",
    "- If the command includes pipes, do it like:\n",
    "- `bash -c \"your command | second command\"`\n",
    "\n",
    "- Note that input and output paths are inside the hdfs\n",
    "- Mapper and reducer paths are inside the main filesystem\n",
    "- Run these commands inside the docker shell:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`hdfs dfs -rm -r -f /user/data/ncdc/output*`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "hadoop jar /usr/hdp/2.6.3.0-235/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-input /user/data/ncdc/txt \\\n",
    "-output /user/data/ncdc/output \\\n",
    "-mapper /bin/cat \\\n",
    "-reducer '/usr/bin/wc -l'\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have this line, everything went ok!:\n",
    "\n",
    "`17/12/23 04:11:33 INFO streaming.StreamJob: Output directory: /user/data/ncdc/output`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check the output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`hdfs dfs -cat /user/data/ncdc/output/*`\n",
    "\n",
    "57920"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will rewrite the job adding a combiner:\n",
    "\n",
    "The combiner will take over the job of reducer: For each task, the word count is calculated\n",
    "Now the reducer will just add the word counts!\n",
    "\n",
    "From the shell, the \"bc\" command will do it for us:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's see what happens until the reducer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for file in /data/ncdc/txt/*; \\ # each file is send to cat separately\n",
    "                                  # and we see what mapper, combiner and reducer does\n",
    "# do cat $file | \\ # mapper\n",
    "# wc -l; done # combiner\n",
    "\n",
    "# Copy the whole command into sandbox (docker) shell:\n",
    "# for file in /data/ncdc/txt/*; do cat $file; wc -l; done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`[hdfs@sandbox-hdp comtrade]$ for file in /data/ncdc/txt/*; do cat $file | wc -l; done # combiner\n",
    "6565\n",
    "6565\n",
    "6554\n",
    "6582\n",
    "6561\n",
    "5474\n",
    "5463\n",
    "6585\n",
    "7571\n",
    "0\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, add the reducer:\n",
    "- \"paste -sd+\" puts a \"+\" sign between the numbers\n",
    "- bc will calculate this formula, and add the numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for file in /data/ncdc/txt/*; do cat $file | wc -l; done | paste -sd+ | bc\n",
    "\n",
    "# 57920"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's convert it to a mapreduce job:\n",
    "First flush the output directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`hdfs dfs -rm -r -f /user/data/ncdc/output*`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then:\n",
    "\n",
    "```bash\n",
    "\n",
    "hadoop jar /usr/hdp/2.6.3.0-235/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-input /user/data/ncdc/txt \\\n",
    "-output /user/data/ncdc/output2 \\\n",
    "-mapper /bin/cat \\\n",
    "-combiner '/usr/bin/wc -l' \\\n",
    "-reducer \"bash -c 'paste -sd+ | bc'\"\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17/12/23 04:29:24 INFO streaming.StreamJob: Output directory: /user/data/ncdc/output2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And see the output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`hdfs dfs -cat /user/data/ncdc/output2/*`\n",
    "\n",
    "57920"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Title Count Across Years Using IMDB Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going over a more detailed problem, you are given an assignment:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will write a map reduce job using the imdb dataset:\n",
    "- The files at the sandbox-hdp filesystem reside at directory /data/imdb/split\n",
    "- These files are the title.basics2.tsv file split into 5 files of 1e6 lines\n",
    "- The same files are imported into the hdfs at directory /user/data/imdb/split\n",
    "\n",
    "You should be user hdfs inside sandbox-hdp:\n",
    "`su hdfs`\n",
    "\n",
    "You can view the files inside hdfs using the command:\n",
    "`hdfs dfs -ls /user/data/imdb/split`\n",
    "\n",
    "`[hdfs@sandbox-hdp split]$ hdfs dfs -ls /user/data/imdb/split\n",
    "Found 5 items\n",
    "-rw-r--r--   1 hdfs hdfs   82470261 2017-12-23 03:54 /user/data/imdb/split/xaa\n",
    "-rw-r--r--   1 hdfs hdfs   84850192 2017-12-23 03:54 /user/data/imdb/split/xab\n",
    "-rw-r--r--   1 hdfs hdfs   85630727 2017-12-23 03:54 /user/data/imdb/split/xac\n",
    "-rw-r--r--   1 hdfs hdfs   84405203 2017-12-23 03:54 /user/data/imdb/split/xad\n",
    "-rw-r--r--   1 hdfs hdfs   45003089 2017-12-23 03:54 /user/data/imdb/split/xae\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You job is to \n",
    "- Get the start year column of the files as the mapper\n",
    "- Get the count of each year as the combiner\n",
    "- Aggregate the count of years in each task as the reducer\n",
    "\n",
    "You can use the \"cut\" command to get the necessary column. Note that default field delimiter for cut is \"\\t\", the same as the files\n",
    "\n",
    "You can view the initial rows of the file with head, so that you decide on which column to extract\n",
    "\n",
    "For combiner and reducer, you can use a small but very talented tool called \"q\" inside sandbox. \"q\" uses sqlite as its backend, however, it does not need a database: It can work on columnar data fed from stdin. Usual sql statements just work!\n",
    "\n",
    "Only you should write \"-\" for the \"from\" clause and fields are named as c1, c2, etc. You can use select, from, group by and aggeragete functions such as count() or sum().\n",
    "\n",
    "Note that when feeding into combiner or reducer you should wrap the line inside quotes. But \n",
    "the statement itself needs quotes. So one of the quote pair should be single and the other should be double as such: -combiner 'q \"select ........\"'\n",
    "\n",
    "The output path you provide must be non-existent. So either provide a new one or flush the existing one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First try the mapper, combiner and reducer on the command line inside the docker shell:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`for file in /data/imdb/split/*; do your_mapper_command | head -3; echo; done`\n",
    "\n",
    "1894\n",
    "1892\n",
    "1892\n",
    "\n",
    "2000\n",
    "2000\n",
    "2000\n",
    "\n",
    "2013\n",
    "2011\n",
    "2011\n",
    "\n",
    "2014\n",
    "1909\n",
    "2014\n",
    "\n",
    "2016\n",
    "2016\n",
    "2016\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we add the combiner (\"head\" and \"column\" are there just for visual purposes):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`for file in /data/imdb/split/*; do your_mapper_command $file | \\\n",
    "q \"your_combiner_sql_statement\" | head -20 | column -c 150; done`\n",
    "\n",
    "`Warning: column count is one - did you provide the correct delimiter?\n",
    "1888 2\t\t1891 7\t\t1894 66\t\t1897 798\t1900 811\t1903 1642\t1906 508\n",
    "1889 1\t\t1892 9\t\t1895 65\t\t1898 1049\t1901 924\t1904 492\t1907 553\n",
    "1890 3\t\t1893 2\t\t1896 466\t1899 893\t1902 879\t1905 307\n",
    "Warning: column count is one - did you provide the correct delimiter?\n",
    "1888 1\t\t1895 3\t\t1898 86\t\t1901 155\t1904 551\t1907 823\t1910 2633\n",
    "1890 1\t\t1896 105\t1899 174\t1902 224\t1905 482\t1908 1689\t1911 3076\n",
    "1894 4\t\t1897 117\t1900 180\t1903 224\t1906 524\t1909 2016\n",
    "Warning: column count is one - did you provide the correct delimiter?\n",
    "1874 1\t\t1888 2\t\t1891 2\t\t1896 250\t1899 719\t1902 702\t1905 892\n",
    "1878 1\t\t1889 1\t\t1894 25\t\t1897 407\t1900 823\t1903 795\t1906 822\n",
    "1887 1\t\t1890 1\t\t1895 47\t\t1898 603\t1901 673\t1904 783\n",
    "Warning: column count is one - did you provide the correct delimiter?\n",
    "1883 1\t\t1896 4\t\t1899 5\t\t1904 1\t\t1908 56\t\t1911 426\t1914 666\n",
    "1890 1\t\t1897 4\t\t1900 9\t\t1905 23\t\t1909 1581\t1912 612\t1915 517\n",
    "1891 1\t\t1898 2\t\t1903 1\t\t1907 7\t\t1910 1465\t1913 473\n",
    "Warning: column count is one - did you provide the correct delimiter?\n",
    "1887 1\t1897 16\t1899 9\t1901 7\t1903 14\t1905 2\t1907 4\t1909 8\t1911 15\t1913 78\n",
    "1896 7\t1898 30\t1900 11\t1902 3\t1904 3\t1906 3\t1908 3\t1910 10\t1912 65\t1914 49\n",
    "`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And last, we add our reducer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`for file in /data/imdb/split/*; do your_mapper_command $file | \\\n",
    "q \"your_combiner_sql_statement\"; done | \\\n",
    "q \"your_reducer_sql_statement\" | column -c 100`\n",
    "\n",
    "`Warning: column count is one - did you provide the correct delimiter?\n",
    "Warning: column count is one - did you provide the correct delimiter?\n",
    "Warning: column count is one - did you provide the correct delimiter?\n",
    "Warning: column count is one - did you provide the correct delimiter?\n",
    "Warning: column count is one - did you provide the correct delimiter?\n",
    "1874 1\t\t1909 5420\t1934 2701\t1959 13273\t1984 23243\t2009 160879\n",
    "1878 1\t\t1910 6410\t1935 2645\t1960 14479\t1985 24729\t2010 178864\n",
    "1883 1\t\t1911 6439\t1936 3081\t1961 14573\t1986 25012\t2011 207730\n",
    "1887 2\t\t1912 8459\t1937 3341\t1962 13627\t1987 26746\t2012 229315\n",
    "1888 5\t\t1913 9562\t1938 3191\t1963 15111\t1988 25883\t2013 242918\n",
    "1889 2\t\t1914 9003\t1939 2828\t1964 16602\t1989 27896\t2014 248265\n",
    "1890 6\t\t1915 8428\t1940 2396\t1965 18361\t1990 29530\t2015 251666\n",
    "1891 10\t\t1916 6959\t1941 2379\t1966 19025\t1991 30996\t2016 253051\n",
    "1892 9\t\t1917 5524\t1942 2277\t1967 19709\t1992 32802\t2017 162619\n",
    "1893 2\t\t1918 4608\t1943 2064\t1968 18150\t1993 35493\t2018 11721\n",
    "1894 95\t\t1919 3974\t1944 1895\t1969 19139\t1994 39719\t2019 975\n",
    "1895 115\t1920 4425\t1945 1855\t1970 19248\t1995 46688\t2020 212\n",
    "1896 832\t1921 4177\t1946 2248\t1971 19507\t1996 47151\t2021 34\n",
    "1897 1342\t1922 3566\t1947 2716\t1972 18904\t1997 51891\t2022 25\n",
    "1898 1770\t1923 3004\t1948 3296\t1973 19964\t1998 58519\t2023 5\n",
    "1899 1800\t1924 3057\t1949 4310\t1974 19458\t1999 62518\t2024 3\n",
    "1900 1834\t1925 3288\t1950 5374\t1975 20122\t2000 66473\t2025 1\n",
    "1901 1759\t1926 3016\t1951 6333\t1976 19101\t2001 74113\t2026 1\n",
    "1902 1808\t1927 3100\t1952 7119\t1977 19296\t2002 77652\t2115 1\n",
    "1903 2676\t1928 3065\t1953 7824\t1978 19401\t2003 86811\tN 321491\n",
    "1904 1830\t1929 3149\t1954 8276\t1979 20030\t2004 102515\n",
    "1905 1706\t1930 2778\t1955 9502\t1980 20921\t2005 115374\n",
    "1906 1857\t1931 2779\t1956 10716\t1981 19853\t2006 129299\n",
    "1907 2481\t1932 2783\t1957 12085\t1982 20390\t2007 143551\n",
    "1908 4274\t1933 2635\t1958 12564\t1983 21264\t2008 151624\n",
    "`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to run it as a mapreduce job on hdfs and yarn, before that we flush the output  directory (or pass a non-existent one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`hdfs dfs -rm -r /user/data/imdb/output*`\n",
    "\n",
    "`rm: `/user/data/imdb/output*': No such file or directory`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And run you mapreduce job:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`hadoop jar /usr/hdp/2.6.3.0-235/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-input /user/data/imdb/split \\\n",
    "-output /user/data/imdb/output \\\n",
    "-mapper \"your_mapper_command\" \\\n",
    "-combiner 'q \"your_combiner_sql_statement\"' \\\n",
    "-reducer 'q \"your_reducer_sql_statement\"'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can track the job from the link using your browser:\n",
    "\n",
    "http://sandbox-hdp.hortonworks.com:8088\n",
    "\n",
    "This is important in order to detect bugs and problems by viewing the logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are also given the exact link for the ongoing job:\n",
    "\n",
    "`17/12/23 06:19:40 INFO mapreduce.Job: The url to track the job: http://sandbox-hdp.hortonworks.com:8088/proxy/application_1513995729254_0004/\n",
    "17/12/23 06:19:40 INFO mapreduce.Job: Running job: job_1513995729254_0004\n",
    "17/12/23 06:19:49 INFO mapreduce.Job: Job job_1513995729254_0004 running in uber mode : false\n",
    "17/12/23 06:19:49 INFO mapreduce.Job:  map 0% reduce 0%\n",
    "17/12/23 06:20:18 INFO mapreduce.Job:  map 5% reduce 0%\n",
    "17/12/23 06:20:21 INFO mapreduce.Job:  map 12% reduce 0%\n",
    "17/12/23 06:20:24 INFO mapreduce.Job:  map 20% reduce 0%\n",
    "17/12/23 06:20:25 INFO mapreduce.Job:  map 22% reduce 0%\n",
    "17/12/23 06:20:28 INFO mapreduce.Job:  map 32% reduce 0%\n",
    "17/12/23 06:20:31 INFO mapreduce.Job:  map 42% reduce 0%\n",
    "17/12/23 06:20:34 INFO mapreduce.Job:  map 48% reduce 0%\n",
    "17/12/23 06:20:37 INFO mapreduce.Job:  map 54% reduce 0%\n",
    "17/12/23 06:20:40 INFO mapreduce.Job:  map 62% reduce 0%\n",
    "17/12/23 06:20:43 INFO mapreduce.Job:  map 67% reduce 0%\n",
    "17/12/23 06:21:09 INFO mapreduce.Job:  map 73% reduce 0%`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are done!:\n",
    "\n",
    "17/12/23 06:21:47 INFO streaming.StreamJob: Output directory: /user/data/imdb/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first check whether output files exist:\n",
    "\n",
    "`[hdfs@sandbox-hdp split]$ hdfs dfs -ls /user/data/imdb/output\n",
    "Found 2 items\n",
    "-rw-r--r--   1 hdfs hdfs          0 2017-12-23 06:21 /user/data/imdb/output/_SUCCESS\n",
    "-rw-r--r--   1 hdfs hdfs       1617 2017-12-23 06:21 /user/data/imdb/output/part-00000`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And see our output with some formatting into columns:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`hdfs dfs -cat /user/data/imdb/output/* | column -c 30`\n",
    "\n",
    "`1874\t 1\t\t1933\t 2635\t\t1982\t 20390\n",
    "1878\t 1\t\t1934\t 2701\t\t1983\t 21264\n",
    "1883\t 1\t\t1935\t 2645\t\t1984\t 23243\n",
    "1887\t 2\t\t1936\t 3081\t\t1985\t 24729\n",
    "1888\t 5\t\t1937\t 3341\t\t1986\t 25012\n",
    "1889\t 2\t\t1938\t 3191\t\t1987\t 26746\n",
    "1890\t 6\t\t1939\t 2828\t\t1988\t 25883\n",
    "1891\t 10\t\t1940\t 2396\t\t1989\t 27896\n",
    "1892\t 9\t\t1941\t 2379\t\t1990\t 29530\n",
    "1893\t 2\t\t1942\t 2277\t\t1991\t 30996\n",
    "1894\t 95\t\t1943\t 2064\t\t1992\t 32802\n",
    "1895\t 115\t\t1944\t 1895\t\t1993\t 35493\n",
    "1896\t 832\t\t1945\t 1855\t\t1994\t 39719\n",
    "1897\t 1342\t\t1946\t 2248\t\t1995\t 46688\n",
    "1898\t 1770\t\t1947\t 2716\t\t1996\t 47151\n",
    "1899\t 1800\t\t1948\t 3296\t\t1997\t 51891\n",
    "1900\t 1834\t\t1949\t 4310\t\t1998\t 58519\n",
    "1901\t 1759\t\t1950\t 5374\t\t1999\t 62518\n",
    "1902\t 1808\t\t1951\t 6333\t\t2000\t 66473\n",
    "1903\t 2676\t\t1952\t 7119\t\t2001\t 74113\n",
    "1904\t 1830\t\t1953\t 7824\t\t2002\t 77652\n",
    "1905\t 1706\t\t1954\t 8276\t\t2003\t 86811\n",
    "1906\t 1857\t\t1955\t 9502\t\t2004\t 102515\n",
    "1907\t 2481\t\t1956\t 10716\t\t2005\t 115374\n",
    "1908\t 4274\t\t1957\t 12085\t\t2006\t 129299\n",
    "1909\t 5420\t\t1958\t 12564\t\t2007\t 143551\n",
    "1910\t 6410\t\t1959\t 13273\t\t2008\t 151624\n",
    "1911\t 6439\t\t1960\t 14479\t\t2009\t 160879\n",
    "1912\t 8459\t\t1961\t 14573\t\t2010\t 178864\n",
    "1913\t 9562\t\t1962\t 13627\t\t2011\t 207730\n",
    "1914\t 9003\t\t1963\t 15111\t\t2012\t 229315\n",
    "1915\t 8428\t\t1964\t 16602\t\t2013\t 242918\n",
    "1916\t 6959\t\t1965\t 18361\t\t2014\t 248265\n",
    "1917\t 5524\t\t1966\t 19025\t\t2015\t 251666\n",
    "1918\t 4608\t\t1967\t 19709\t\t2016\t 253051\n",
    "1919\t 3974\t\t1968\t 18150\t\t2017\t 162619\n",
    "1920\t 4425\t\t1969\t 19139\t\t2018\t 11721\n",
    "1921\t 4177\t\t1970\t 19248\t\t2019\t 975\n",
    "1922\t 3566\t\t1971\t 19507\t\t2020\t 212\n",
    "1923\t 3004\t\t1972\t 18904\t\t2021\t 34\n",
    "1924\t 3057\t\t1973\t 19964\t\t2022\t 25\n",
    "1925\t 3288\t\t1974\t 19458\t\t2023\t 5\n",
    "1926\t 3016\t\t1975\t 20122\t\t2024\t 3\n",
    "1927\t 3100\t\t1976\t 19101\t\t2025\t 1\n",
    "1928\t 3065\t\t1977\t 19296\t\t2026\t 1\n",
    "1929\t 3149\t\t1978\t 19401\t\t2115\t 1\n",
    "1930\t 2778\t\t1979\t 20030\t\tN\t 321491\n",
    "1931\t 2779\t\t1980\t 20921\n",
    "1932\t 2783\t\t1981\t 19853`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same as the output from the shell command!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Scripts for MapReduce Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Temperature Example on NCDC Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using the same NCDC data set we'll recreate the maximum temperature example from the Elephant Book as a \"Hadoop streaming\" job:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your computers have a century of the temperature data of USA. We will use just the first 10 years of this data\n",
    "\n",
    "And for each year we will get the max temperature\n",
    "\n",
    "We will first start with standard unix tools\n",
    "\n",
    "I played a bit with the original script given as a part of the supplementary material for the Elephant Book\n",
    "\n",
    "Note that there may be empty files and the job must have a remedy for this issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "#!/usr/bin/env bash\n",
    "\n",
    "# adjusted by Serhat Cevikel to measure the time\n",
    "\n",
    "#START=$(date +%s.%N)\n",
    "path=$1\n",
    "starty=$2\n",
    "endy=$3\n",
    "\n",
    "years=$(seq $starty $endy)\n",
    "\n",
    "\n",
    "for year in $years\n",
    "do\n",
    "    filee=\"${path}/${year}\"\n",
    "  echo -ne `basename $year .gz`\"\\t\"\n",
    "  gunzip -c $filee | \\ \n",
    "    awk '{ temp = substr($0, 88, 5) + 0;\n",
    "           q = substr($0, 93, 1);\n",
    "           if (temp !=9999 && q ~ /[01459]/ && temp > max) max = temp }\n",
    "         END { print max }'\n",
    "done\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing temp values are coded as 9999 and they are excluded\n",
    "\"q\" is a quality code and should be one of 0,1,4,5,9 to be included"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The path to script inside sandbox-hdp is /data/codes/hadoop_max_temperature.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my implementation it takes 3 parameters: the path to gz files, start year and end year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data resides at /data/ncdc/gz/ and /data/ncdc/txt/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do it for 1901 to 1925"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "/data/codes/hadoop_max_temperature.sh \\\n",
    "/data/ncdc/gz \\\n",
    "1901 1910\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "1901\t317\n",
    "1902\t244\n",
    "1903\t289\n",
    "1904\t256\n",
    "1905\t283\n",
    "1906\t294\n",
    "1907\t283\n",
    "1908\t289\n",
    "1909\t278\n",
    "1910\t\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's transform it to a map reduce job "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the map phase, the key value pairs are extracted from the data: the year and temp reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the reduce phase the max temp for each year is calculated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```R\n",
    "#!/usr/bin/Rscript\n",
    "\n",
    "con <- file(\"stdin\")\n",
    "#con <- file(\"1910\")\n",
    "liness <- readLines(con)\n",
    "close(con)\n",
    "\n",
    "year <- as.numeric(substr(liness, 16, 19))\n",
    "temp <- as.numeric(substr(liness, 88, 92))\n",
    "qq <- as.numeric(substr(liness, 93, 93))\n",
    "\n",
    "output <- cbind(year, temp)\n",
    "\n",
    "output <- output[temp != 9999 & qq %in% c(0, 1, 4, 5, 9),]\n",
    "\n",
    "for (i in seq_along(output[,1]))\n",
    "{\n",
    "    pasted <- paste(output[i,], collapse = \"\\t\")\n",
    "    cat(sprintf(\"%s\\n\", pasted))\n",
    "}\n",
    "```                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code accepts input from stdin so can be used similar to the previous one - before Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The path to code inside sandbox-hdp is /data/codes/mapper.R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cat /data/ncdc/txt/{1901..1910} | /data/codes/mapper.R | head`\n",
    "```\n",
    "1901\t-78\n",
    "1901\t-72\n",
    "1901\t-94\n",
    "1901\t-61\n",
    "1901\t-56\n",
    "1901\t-28\n",
    "1901\t-67\n",
    "1901\t-33\n",
    "1901\t-28\n",
    "1901\t-33\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reducer code is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```R\n",
    "#!/usr/bin/Rscript\n",
    "\n",
    "con <- file(\"stdin\")\n",
    "#com <- file(\"mapped\")\n",
    "liness <- readLines(con)\n",
    "close(con)\n",
    "\n",
    "keyval <- list()\n",
    "\n",
    "for (i in seq_along(liness))\n",
    "{\n",
    "    linex <- unlist(strsplit(liness[i], split = \"\\t\"))\n",
    "    key <- linex[1]\n",
    "    val <- as.numeric(linex[2])\n",
    "\n",
    "    cur.maxval <- keyval[[key]]\n",
    "\n",
    "    if (is.null(cur.maxval))\n",
    "    {   \n",
    "        keyval[[key]] <- val \n",
    "    }\n",
    "    else\n",
    "    {   \n",
    "        if (val > cur.maxval)\n",
    "        {\n",
    "            keyval[[key]] <- val \n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "keys <- as.numeric(names(keyval))\n",
    "vals <- as.numeric(unlist(keyval))\n",
    "\n",
    "output <- matrix(c(keys, vals), ncol = 2)\n",
    "output <- output[order(keys),, drop = F]\n",
    "\n",
    "for (i in seq_along(output[,1]))\n",
    "{\n",
    "    pasted <- paste(output[i,], collapse = \"\\t\")\n",
    "    cat(sprintf(\"%s\\n\", pasted))\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The path to the reducer code is /home/bda505/mef-bigdata/week_07/codes/reducer.R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cat /data/ncdc/txt/{1901..1910} | /data/codes/mapper.R | /data/codes/reducer.R`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "1901\t317\n",
    "1902\t244\n",
    "1903\t289\n",
    "1904\t256\n",
    "1905\t283\n",
    "1906\t294\n",
    "1907\t283\n",
    "1908\t289\n",
    "1909\t278\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the map reduce job. Note that we have to pass the custom script files via \"-file\" option so that all nodes can run it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "hadoop jar /usr/hdp/2.6.3.0-235/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-input /user/data/ncdc/txt \\\n",
    "-output /user/data/ncdc/output16 \\\n",
    "-mapper /data/codes/mapper.R \\\n",
    "-reducer /data/codes/reducer.R \\\n",
    "-file /data/codes/mapper.R \\\n",
    "-file /data/codes/reducer.R\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can run it with a combiner (the same script as the reducer for this case):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "\n",
    "hadoop jar /usr/hdp/2.6.3.0-235/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "-input /user/data/ncdc/txt \\\n",
    "-output /user/data/ncdc/output16 \\\n",
    "-mapper /data/codes/mapper.R \\\n",
    "-combiner reducer.R \\\n",
    "-reducer /data/codes/reducer.R \\\n",
    "-file /data/codes/mapper.R \\\n",
    "-file /data/codes/reducer.R\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {
    "height": "239px",
    "left": "36px",
    "right": "941.167px",
    "top": "110px",
    "width": "143px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
