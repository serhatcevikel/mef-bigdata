{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#SQOOP-FOR-DATA-IMPORTING\" data-toc-modified-id=\"SQOOP-FOR-DATA-IMPORTING-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>SQOOP FOR DATA IMPORTING</a></span><ul class=\"toc-item\"><li><span><a href=\"#IMPORT-AS-TEXT-FILES\" data-toc-modified-id=\"IMPORT-AS-TEXT-FILES-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>IMPORT AS TEXT FILES</a></span></li><li><span><a href=\"#IMPORT-AS-HIVE-TABLES\" data-toc-modified-id=\"IMPORT-AS-HIVE-TABLES-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>IMPORT AS HIVE TABLES</a></span></li><li><span><a href=\"#AN-IMPORT-EXERCISE\" data-toc-modified-id=\"AN-IMPORT-EXERCISE-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>AN IMPORT EXERCISE</a></span></li></ul></li><li><span><a href=\"#HIVE\" data-toc-modified-id=\"HIVE-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>HIVE</a></span><ul class=\"toc-item\"><li><span><a href=\"#CREATE/DROP-DATABASES,-MOVE-AND-COPY-TABLES\" data-toc-modified-id=\"CREATE/DROP-DATABASES,-MOVE-AND-COPY-TABLES-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>CREATE/DROP DATABASES, MOVE AND COPY TABLES</a></span></li><li><span><a href=\"#A-HIVE-EXERCISE\" data-toc-modified-id=\"A-HIVE-EXERCISE-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>A HIVE EXERCISE</a></span></li></ul></li><li><span><a href=\"#PIG\" data-toc-modified-id=\"PIG-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>PIG</a></span><ul class=\"toc-item\"><li><span><a href=\"#PIG-TUTORIAL\" data-toc-modified-id=\"PIG-TUTORIAL-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>PIG TUTORIAL</a></span></li><li><span><a href=\"#PIG-EXERCISE\" data-toc-modified-id=\"PIG-EXERCISE-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>PIG EXERCISE</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQOOP FOR DATA IMPORTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use sqoop in order to import data\n",
    "- from RDBMS\n",
    "- into HDFS as text files\n",
    "- or as hive tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we will run all shell commands inside the docker via ssh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So first let's start an ssh connection into our sandbox-hdp docker container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ssh root@localhost -p 2223"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, in order to be sure we are inside the sandbox-hdp docker, check the shell prompt. It should be as such:\n",
    "\n",
    "`root@sandbox-hdp ~]#`\n",
    "\n",
    "The hash sign \"#\" tells that we are now the root user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imdb database that we are familiar from SQL sessions is imported into the postgresql server running inside the docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to some limitations for X server display from inside the docker, pgadmin3 cannot run.\n",
    "\n",
    "So we will prob our postgresql databases only through the command line client \"psql\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do that we will first change the system user to postgres\n",
    "\n",
    "Note that this is not the same as the database user \"postgres\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# su postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the prompt changed to:\n",
    "\n",
    "`bash-4.1$`\n",
    "\n",
    "The \"$\" tells that we are a normal user again - not the root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to connect to a database acting as a database user from the command line, the command is:\n",
    "\n",
    "`psql dbname dbuser`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# psql imdb postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the prompt should be:\n",
    "\n",
    "`imdb=#`\n",
    "\n",
    "We can now query the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's see whether all tables are imported.\n",
    "\n",
    "The command to list the tables in a database is:\n",
    "\n",
    "`\\dt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \\dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should be:\n",
    "\n",
    "```bash\n",
    "imdb=# \\dt\n",
    "                 List of relations\n",
    " Schema |         Name          | Type  |  Owner   \n",
    "--------+-----------------------+-------+----------\n",
    " public | name_basics           | table | postgres\n",
    " public | title_basics          | table | postgres\n",
    " public | title_crew            | table | postgres\n",
    " public | title_episode         | table | postgres\n",
    " public | title_principals_melt | table | postgres\n",
    " public | title_ratings         | table | postgres\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we remember from our first sessions (you can check week_02_commands file),\n",
    "\n",
    "In order to get the total size of the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELECT pg_size_pretty(pg_database_size('imdb'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should be:\n",
    "\n",
    "```bash\n",
    "imdb=# SELECT pg_size_pretty(pg_database_size('imdb'));\n",
    " pg_size_pretty \n",
    "----------------\n",
    " 4538 MB\n",
    "(1 row)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can input any SQL statement that we have created so far directly into the psql prompt.\n",
    "\n",
    "For example, remember the query:\n",
    "\n",
    "- Starting with first option (not the subquery), let's make a three way join\n",
    "- First join titles and principal cast on title id's (tconst)\n",
    "- And then join principal cast and name basics on name id's (nconst)\n",
    "- Filter only for actors and actresses\n",
    "- And sort on first names (ascendng) then title years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "SELECT title_basics.tconst, title_basics.originaltitle, title_basics.startyear, title_basics.runtimeminutes,\n",
    "\ttitle_basics.genres, title_principals_melt.principalcast, name_basics.primaryname,\n",
    "\tname_basics.birthyear, name_basics.deathyear, name_basics.primaryprofession\n",
    "\n",
    "FROM title_basics LEFT JOIN title_principals_melt ON title_basics.tconst=title_principals_melt.tconst\n",
    "\tLEFT JOIN name_basics ON title_principals_melt.principalcast=name_basics.nconst\n",
    "\n",
    "WHERE title_basics.originaltitle ~ '.*Godfather.*Part.*'\n",
    "\tAND title_basics.genres ~ '(?i)drama'\n",
    "\tAND NOT title_basics.genres ~ '(?i)comedy'\n",
    "\tAND title_basics.startyear <= 1990\n",
    "\tAND name_basics.primaryprofession ~'actor|actress'\n",
    "\n",
    "ORDER BY name_basics.primaryname, title_basics.startyear DESC;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should be:\n",
    "\n",
    "```bash\n",
    "  tconst   |      originaltitle      | startyear | runtimeminutes |   genres    | principalcast |  primaryname   | birthyear | deathyear |     primaryprofession     \n",
    "-----------+-------------------------+-----------+----------------+-------------+---------------+----------------+-----------+-----------+---------------------------\n",
    " tt0099674 | The Godfather: Part III |      1990 |            162 | Crime,Drama | nm0000199     | Al Pacino      |      1940 |           | actor,soundtrack,director\n",
    " tt0071562 | The Godfather: Part II  |      1974 |            202 | Crime,Drama | nm0000199     | Al Pacino      |      1940 |           | actor,soundtrack,director\n",
    " tt0099674 | The Godfather: Part III |      1990 |            162 | Crime,Drama | nm0000412     | Andy Garcia    |      1956 |           | actor,producer,soundtrack\n",
    " tt0099674 | The Godfather: Part III |      1990 |            162 | Crime,Drama | nm0000473     | Diane Keaton   |      1946 |           | actress,producer,director\n",
    " tt0071562 | The Godfather: Part II  |      1974 |            202 | Crime,Drama | nm0000473     | Diane Keaton   |      1946 |           | actress,producer,director\n",
    " tt0071562 | The Godfather: Part II  |      1974 |            202 | Crime,Drama | nm0000134     | Robert De Niro |      1943 |           | actor,producer,soundtrack\n",
    " tt0071562 | The Godfather: Part II  |      1974 |            202 | Crime,Drama | nm0000380     | Robert Duvall  |      1931 |           | actor,producer,soundtrack\n",
    " tt0099674 | The Godfather: Part III |      1990 |            162 | Crime,Drama | nm0001735     | Talia Shire    |      1946 |           | actress,producer,director\n",
    "(8 rows)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are pretty sure that, our database is completely inside the docker\n",
    "\n",
    "Let's start to import via sqoop!\n",
    "\n",
    "First let's leave the psql shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \\q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's leave postgres user and shift to hdfs user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exit\n",
    "# su hdfs\n",
    "# cd /"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the prompt is:\n",
    "\n",
    "`[hdfs@sandbox-hdp /]$`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order the connect to a database server the relevant connect driver (JDBC) must be downloaded to the correct destination\n",
    "\n",
    "(note that this step is already done in our example. This is for your reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The link for the current JDBC file is:\n",
    "\n",
    "https://jdbc.postgresql.org/download/postgresql-42.1.4.jar\n",
    "\n",
    "And the path for the file is (inside the sandbox-hdp docker):\n",
    "\n",
    "`/usr/hdp/current/sqoop-client/lib/`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, we configured the postgresql server inside the docker so that it will not prompt for password, just for convenience.\n",
    "\n",
    "This is not a secure way in a production environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's first see whether sqoop can connect to our postgresql database and list the tables:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "sqoop list-tables --connect jdbc:postgresql://localhost:5432/imdb --username postgres\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should be:\n",
    "```bash\n",
    "\n",
    "name_basics\n",
    "title_basics\n",
    "title_crew\n",
    "title_episode\n",
    "title_principals_melt\n",
    "title_ratings\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, sqoop recognizes and connects to postgresql server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORT AS TEXT FILES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, first import a single table as a text file into hdfs\n",
    "\n",
    "The target directory should be non-existent. The direct flag is for fast imports:\n",
    "\n",
    "```bash\n",
    "\n",
    "sqoop import --connect jdbc:postgresql://localhost:5432/imdb \\\n",
    "--username postgres \\\n",
    "--table name_basics \\\n",
    "--target-dir /user/data/imdb_import_1 \\\n",
    "--direct\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should be:\n",
    "\n",
    "```bash\n",
    "\n",
    "18/01/04 00:55:35 INFO manager.DirectPostgresqlManager: Performing import of table name_basics from database imdb\n",
    "18/01/04 00:56:02 INFO manager.DirectPostgresqlManager: Transfer loop complete.\n",
    "18/01/04 00:56:02 INFO manager.DirectPostgresqlManager: Transferred 452.8851 MB in 24.596 seconds (18.413 MB/sec)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check whether the new directory and file(s) exist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdfs dfs -ls /user/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`drwxr-xr-x   - hdfs hdfs          0 2018-01-04 00:55 /user/data/imdb_import_1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdfs dfs -ls /user/data/imdb_import_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`-rw-r--r--   1 hdfs hdfs  475483887 2018-01-04 00:56 /user/data/imdb_import_1/part-m-00000`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file is there. Let's read the head of the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdfs dfs -cat /user/data/imdb_import_1/* | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "nm0000001,Fred Astaire,1899,1987,\"soundtrack,actor,miscellaneous\",\"tt0120689,tt0027125,tt0028333,tt0050419\"\n",
    "nm0000002,Lauren Bacall,1924,2014,\"actress,soundtrack\",\"tt0038355,tt0040506,tt0055688,tt0037382\"\n",
    "nm0000003,Brigitte Bardot,1934,,\"actress,soundtrack,producer\",\"tt0049189,tt0057345,tt0059956,tt0063715\"\n",
    "nm0000004,John Belushi,1949,1982,\"actor,writer,soundtrack\",\"tt0077975,tt0078723,tt0080455,tt0072562\"\n",
    "nm0000005,Ingmar Bergman,1918,2007,\"writer,director,actor\",\"tt0060827,tt0050976,tt0050986,tt0083922\"\n",
    "nm0000006,Ingrid Bergman,1915,1982,\"actress,soundtrack,producer\",\"tt0038787,tt0034583,tt0038109,tt0077711\"\n",
    "nm0000007,Humphrey Bogart,1899,1957,\"actor,soundtrack,producer\",\"tt0034583,tt0040897,tt0033870,tt0038355\"\n",
    "nm0000008,Marlon Brando,1924,2004,\"actor,soundtrack,director\",\"tt0078788,tt0047296,tt0068646,tt0078346\"\n",
    "nm0000009,Richard Burton,1925,1984,\"actor,producer,soundtrack\",\"tt0061184,tt0057877,tt0087803,tt0065207\"\n",
    "nm0000010,James Cagney,1899,1986,\"actor,soundtrack,director\",\"tt0042041,tt0029870,tt0031867,tt0035575\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, by default the imported format for text is comma separated values.\n",
    "\n",
    "We can also import the file into binary formats (avro, parquet or sequence files)\n",
    "\n",
    "In order to get more info on import options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sqoop help import | less"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing a single table is OK. But what if we want to import multiple tables at once?\n",
    "\n",
    "sqoop has another command \"import-all-tables\"\n",
    "\n",
    "First let's flush the import directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdfs dfs -rm -r /user/data/imdb_import_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And check the contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdfs dfs -ls /user/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now import all tables. Not that the option for the target path is now \"--warehouse-dir\":\n",
    "\n",
    "```bash\n",
    "\n",
    "sqoop import-all-tables \\\n",
    "--connect jdbc:postgresql://localhost:5432/imdb \\\n",
    "--username postgres \\\n",
    "--warehouse-dir /user/data/imdb_import_1 \\\n",
    "--direct\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the files from hdfs. Note that, a separate directory is created for each table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdfs dfs -ls /user/data/imdb_import_1/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "-rw-r--r--   1 hdfs hdfs  475483887 2018-01-04 01:07 /user/data/imdb_import_1/name_basics/part-m-00000\n",
    "-rw-r--r--   1 hdfs hdfs  475483887 2018-01-04 00:56 /user/data/imdb_import_1/part-m-00000\n",
    "Found 1 items\n",
    "-rw-r--r--   1 hdfs hdfs  387758108 2018-01-04 01:07 /user/data/imdb_import_1/title_basics/part-m-00000\n",
    "Found 1 items\n",
    "-rw-r--r--   1 hdfs hdfs  133155566 2018-01-04 01:07 /user/data/imdb_import_1/title_crew/part-m-00000\n",
    "Found 1 items\n",
    "-rw-r--r--   1 hdfs hdfs   72331204 2018-01-04 01:07 /user/data/imdb_import_1/title_episode/part-m-00000\n",
    "Found 1 items\n",
    "-rw-r--r--   1 hdfs hdfs  507177900 2018-01-04 01:08 /user/data/imdb_import_1/title_principals_melt/part-m-00000\n",
    "Found 1 items\n",
    "-rw-r--r--   1 hdfs hdfs   13053098 2018-01-04 01:08 /user/data/imdb_import_1/title_ratings/part-m-00000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can control the number of map tasks to import the data with the \"-m\" flag. The higher the number is, separate files will be created for each map task inside the target directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORT AS HIVE TABLES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that, Hive is a data warehouse system that provides a SQL like interface to query the data inside the HDFS.\n",
    "\n",
    "Sqoop can import the data as Hive tables with \"--hive-import --create-hive-table\" flags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "sqoop import \\\n",
    "--connect jdbc:postgresql://localhost:5432/imdb \\\n",
    "--username postgres \\\n",
    "--table name_basics \\\n",
    "--hive-import --create-hive-table --direct\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should be:\n",
    "```bash\n",
    "Time taken: 3.905 seconds\n",
    "Loading data to table default.name_basics\n",
    "Table default.name_basics stats: [numFiles=1, numRows=0, totalSize=466334419, rawDataSize=0]\n",
    "OK\n",
    "Time taken: 2.267 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But where is the file?\n",
    "\n",
    "It is here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdfs dfs -ls /apps/hive/warehouse/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "\n",
    "Found 5 items\n",
    "drwxrwxrwx   - hive hadoop          0 2017-11-10 14:59 /apps/hive/warehouse/foodmart.db\n",
    "drwxrwxrwx   - hdfs hadoop          0 2018-01-04 01:28 /apps/hive/warehouse/name_basics\n",
    "drwxrwxrwx   - hive hadoop          0 2017-11-10 15:00 /apps/hive/warehouse/sample_07\n",
    "drwxrwxrwx   - hive hadoop          0 2017-11-10 15:00 /apps/hive/warehouse/sample_08\n",
    "drwxrwxrwx   - hive hadoop          0 2017-11-10 14:53 /apps/hive/warehouse/xademo.db\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can check from hive that the table is imported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hive -e 'show tables'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "OK\n",
    "name_basics\n",
    "sample_07\n",
    "sample_08\n",
    "Time taken: 3.173 seconds, Fetched: 3 row(s)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to read the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdfs dfs -cat /apps/hive/warehouse/name_basics/* | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See that, it is not a pure text file anymore:\n",
    "\n",
    "```bash\n",
    "nm0000001\u0001Fred Astaire\u00011899\u00011987\u0001soundtrack,actor,miscellaneous\u0001tt0120689,tt0027125,tt0028333,tt0050419\n",
    "nm0000002\u0001Lauren Bacall\u00011924\u00012014\u0001actress,soundtrack\u0001tt0038355,tt0040506,tt0055688,tt0037382\n",
    "nm0000003\u0001Brigitte Bardot\u00011934\u0001\u0001actress,soundtrack,producer\u0001tt0049189,tt0057345,tt0059956,tt0063715\n",
    "nm0000004\u0001John Belushi\u00011949\u00011982\u0001actor,writer,soundtrack\u0001tt0077975,tt0078723,tt0080455,tt0072562\n",
    "nm0000005\u0001Ingmar Bergman\u00011918\u00012007\u0001writer,director,actor\u0001tt0060827,tt0050976,tt0050986,tt0083922\n",
    "nm0000006\u0001Ingrid Bergman\u00011915\u00011982\u0001actress,soundtrack,producer\u0001tt0038787,tt0034583,tt0038109,tt0077711\n",
    "nm0000007\u0001Humphrey Bogart\u00011899\u00011957\u0001actor,soundtrack,producer\u0001tt0034583,tt0040897,tt0033870,tt0038355\n",
    "nm0000008\u0001Marlon Brando\u00011924\u00012004\u0001actor,soundtrack,director\u0001tt0078788,tt0047296,tt0068646,tt0078346\n",
    "nm0000009\u0001Richard Burton\u00011925\u00011984\u0001actor,producer,soundtrack\u0001tt0061184,tt0057877,tt0087803,tt0065207\n",
    "nm0000010\u0001James Cagney\u00011899\u00011986\u0001actor,soundtrack,director\u0001tt0042041,tt0029870,tt0031867,tt0035575\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to delete the table, we do it from the hive command, not by manually deleting the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hive -e 'drop table name_basics'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "OK\n",
    "Time taken: 4.542 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hive -e 'show tables'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No more name_basics:\n",
    "\n",
    "```bash\n",
    "OK\n",
    "sample_07\n",
    "sample_08\n",
    "Time taken: 4.746 seconds, Fetched: 2 row(s)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdfs dfs -ls /apps/hive/warehouse/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And no more name_basics directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's import all tables into hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "sqoop import-all-tables \\\n",
    "--connect jdbc:postgresql://localhost:5432/imdb \\\n",
    "--username postgres \\\n",
    "--hive-import --create-hive-table --direct\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check whether they are imported into hive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hive -e 'show tables'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "OK\n",
    "name_basics\n",
    "sample_07\n",
    "sample_08\n",
    "title_basics\n",
    "title_crew\n",
    "title_episode\n",
    "title_principals_melt\n",
    "title_ratings\n",
    "Time taken: 2.768 seconds, Fetched: 8 row(s)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdfs dfs -ls /apps/hive/warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "Found 10 items\n",
    "drwxrwxrwx   - hive hadoop          0 2017-11-10 14:59 /apps/hive/warehouse/foodmart.db\n",
    "drwxrwxrwx   - hdfs hadoop          0 2018-01-04 01:41 /apps/hive/warehouse/name_basics\n",
    "drwxrwxrwx   - hive hadoop          0 2017-11-10 15:00 /apps/hive/warehouse/sample_07\n",
    "drwxrwxrwx   - hive hadoop          0 2017-11-10 15:00 /apps/hive/warehouse/sample_08\n",
    "drwxrwxrwx   - hdfs hadoop          0 2018-01-04 01:42 /apps/hive/warehouse/title_basics\n",
    "drwxrwxrwx   - hdfs hadoop          0 2018-01-04 01:42 /apps/hive/warehouse/title_crew\n",
    "drwxrwxrwx   - hdfs hadoop          0 2018-01-04 01:43 /apps/hive/warehouse/title_episode\n",
    "drwxrwxrwx   - hdfs hadoop          0 2018-01-04 01:43 /apps/hive/warehouse/title_principals_melt\n",
    "drwxrwxrwx   - hdfs hadoop          0 2018-01-04 01:44 /apps/hive/warehouse/title_ratings\n",
    "drwxrwxrwx   - hive hadoop          0 2017-11-10 14:53 /apps/hive/warehouse/xademo.db\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AN IMPORT EXERCISE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you'll \n",
    "- first import a publicly available sql dump into postgresql (inside sandbox-hdp)\n",
    "- Then import the postgresql database into hdfs as text files and hive tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The database is \"World\", containing list of cities, countries and languages. The link is\n",
    "\n",
    "http://pgfoundry.org/frs/download.php/527/world-1.0.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First you should exit hdfs user and become postgres user. Note that the dash \"-\" after \"su\" takes the user to its home directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exit\n",
    "# su - postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download the file using:\n",
    "\n",
    "`wget _url_`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And extract the tar.gz archive using:\n",
    "\n",
    "`tar -xzvf _tar.gz_`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Navigate to dbsamples-0.1/world with cd\n",
    "\n",
    "There you will see world.sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in two simple commands we will create an empty database named world, and import the sql dump into that database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# createdb world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# psql world < world.sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now connect to the database through psql:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# psql world postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And list the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \\dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "world=# \\dt\n",
    "              List of relations\n",
    " Schema |      Name       | Type  |  Owner   \n",
    "--------+-----------------+-------+----------\n",
    " public | city            | table | postgres\n",
    " public | country         | table | postgres\n",
    " public | countrylanguage | table | postgres\n",
    "(3 rows)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the columns in a table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \\d+ city"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "world-# \\d+ city\n",
    "                       Table \"public.city\"\n",
    "   Column    |     Type     | Modifiers | Storage  | Description \n",
    "-------------+--------------+-----------+----------+-------------\n",
    " id          | integer      | not null  | plain    | \n",
    " name        | text         | not null  | extended | \n",
    " countrycode | character(3) | not null  | extended | \n",
    " district    | text         | not null  | extended | \n",
    " population  | integer      | not null  | plain    | \n",
    "Indexes:\n",
    "    \"city_pkey\" PRIMARY KEY, btree (id)\n",
    "Referenced by:\n",
    "    TABLE \"country\" CONSTRAINT \"country_capital_fkey\" FOREIGN KEY (capital) REFERENCES city(id)\n",
    "Has OIDs: no\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's view the head of the tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select * from city limit 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "world=# select * from city limit 10;\n",
    " id |      name      | countrycode |   district    | population \n",
    "----+----------------+-------------+---------------+------------\n",
    "  1 | Kabul          | AFG         | Kabol         |    1780000\n",
    "  2 | Qandahar       | AFG         | Qandahar      |     237500\n",
    "  3 | Herat          | AFG         | Herat         |     186800\n",
    "  4 | Mazar-e-Sharif | AFG         | Balkh         |     127800\n",
    "  5 | Amsterdam      | NLD         | Noord-Holland |     731200\n",
    "  6 | Rotterdam      | NLD         | Zuid-Holland  |     593321\n",
    "  7 | Haag           | NLD         | Zuid-Holland  |     440900\n",
    "  8 | Utrecht        | NLD         | Utrecht       |     234323\n",
    "  9 | Eindhoven      | NLD         | Noord-Brabant |     201843\n",
    " 10 | Tilburg        | NLD         | Noord-Brabant |     193238\n",
    "(10 rows)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also do it for country and countrylanguage tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the rest of the task is:\n",
    "\n",
    "- Exit psql shell\n",
    "- Exit postgres user and become hdfs user again\n",
    "- First check that sqoop can list the tables in the database\n",
    "\n",
    "- Then import all tables as text files into hdfs\n",
    "- Check that they are imported using hdfs commands (list or cat the head)\n",
    "\n",
    "- Then import all tables as hive tables, \n",
    "- Check that they are imported using hive command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HIVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hive makes it possible to run queries on datasets residing in hdfs using an SQL like language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hive can be run:\n",
    "- Interactively through its shell\n",
    "- Non-interactively as inline commands\n",
    "- Or in batch mode through hive scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have imported tables, we can run queries against the tables\n",
    "\n",
    "First let's run it non-interactively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hive -e \"show tables\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "OK\n",
    "city\n",
    "country\n",
    "countrylanguage\n",
    "sample_07\n",
    "sample_08\n",
    "title_basics\n",
    "title_crew\n",
    "title_episode\n",
    "title_principals_melt\n",
    "title_ratings\n",
    "Time taken: 4.137 seconds, Fetched: 10 row(s)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for handling more iterative steps, let's start the hive prompt and run interactively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will turn on the \"hive\" prompt:\n",
    "\n",
    "`hive>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that,\n",
    "- commands should end with semicolon \";\"\n",
    "- they are case insensitive.\n",
    "- and HiveQL closely follows the flavor of MySQL\n",
    "\n",
    "\n",
    "Let's first list the tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mysql\n",
    "show tables;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "city\n",
    "country\n",
    "countrylanguage\n",
    "sample_07\n",
    "sample_08\n",
    "title_basics\n",
    "title_crew\n",
    "title_episode\n",
    "title_principals_melt\n",
    "title_ratings\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way, our tables are not well organized. It is better that we create two databases and attach the tables to relavant databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hive prompt throws an \"OK\" after each successful command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATE/DROP DATABASES, MOVE AND COPY TABLES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first create a database named \"deneme\" from hive prompt:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mysql\n",
    "create database deneme;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And list the databases:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mysql\n",
    "show databases;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "hive> show databases;\n",
    "OK\n",
    "default\n",
    "deneme\n",
    "foodmart\n",
    "xademo\n",
    "Time taken: 0.059 seconds, Fetched: 4 row(s)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's delete the database:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mysql\n",
    "drop database deneme;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mysql\n",
    "show databases;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "hive> show databases;\n",
    "OK\n",
    "default\n",
    "foodmart\n",
    "xademo\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a database for holding imdb tables together:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mysql\n",
    "create database imdb;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can either move the tables into imdb or create copies of them inside imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, first let's create copies of the tables inside imdb database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mysql\n",
    "create table imdb.name_basics\n",
    "as select * from name_basics;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mysql\n",
    "create table imdb.title_basics\n",
    "as select * from title_basics;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mysql\n",
    "create table imdb.title_crew\n",
    "as select * from title_crew;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mysql\n",
    "create table imdb.title_episode\n",
    "as select * from title_episode;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mysql\n",
    "create table imdb.title_principals_melt\n",
    "as select * from title_principals_melt;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mysql\n",
    "create table imdb.title_ratings\n",
    "as select * from title_ratings;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can list the tables in imdb database:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mysql\n",
    "show tables in imdb;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mysql\n",
    "hive> show tables in imdb;\n",
    "OK\n",
    "name_basics\n",
    "title_basics\n",
    "title_crew\n",
    "title_episode\n",
    "title_principals_melt\n",
    "title_ratings\n",
    "Time taken: 0.483 seconds, Fetched: 6 row(s)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now can run some queries inside the database. First let's check whether the copying is complete\n",
    "\n",
    "With the \"use DBNAME;\" statement, the DBNAME acts as the namespace for tables, hence compact the queries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mysql\n",
    "use imdb; select count(*) from name_basics;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mysql\n",
    "hive> use imdb; select count(*) from name_basics;\n",
    "OK\n",
    "Time taken: 0.335 seconds\n",
    "OK\n",
    "8155447\n",
    "Time taken: 0.576 seconds, Fetched: 1 row(s)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can repeat the query for other tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a second database named imdb_new and MOVE the tables in imdb to this new database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mysql\n",
    "create database imdb_new;\n",
    "show databases;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mysql\n",
    "alter table imdb.name_basics\n",
    "rename to imdb_new.name_basics;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check whether name_basics is moved:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mysql\n",
    "hive> show tables in imdb;\n",
    "OK\n",
    "title_basics\n",
    "title_crew\n",
    "title_episode\n",
    "title_principals_melt\n",
    "title_ratings\n",
    "Time taken: 0.47 seconds, Fetched: 5 row(s)\n",
    "hive> show tables in imdb_new;\n",
    "OK\n",
    "name_basics\n",
    "Time taken: 0.454 seconds, Fetched: 1 row(s)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can either repeat it for other tables, or roll back into imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mysql\n",
    "alter table imdb_new.name_basics\n",
    "rename to imdb.name_basics;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a hive database on hdfs, similar to our postgresql database, we can run similar queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:\n",
    "- get the original titles, start year and runtime minutes of\n",
    "- titles before 1930 and\n",
    "- longer than 100 minutes\n",
    "- genres include Drama\n",
    "- limit to the first 10 results\n",
    "\n",
    "Note that instead of tilde (~), we should use LIKE in order to make a partial match in string fields to account for the flavor difference b/w PostgreSQL and MySQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mysql\n",
    "use imdb;\n",
    "\n",
    "SELECT originaltitle, startyear, runtimeminutes\n",
    "  FROM title_basics\n",
    "  WHERE startyear <= 1930\n",
    "  AND runtimeminutes > 100\n",
    "  AND genres LIKE 'Drama'\n",
    "  LIMIT 10;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Atlantis\t1913\t121\n",
    "Germinal\t1913\t150\n",
    "Les misérables - Époque 2: Fantine\t1913\t300\n",
    "Stingaree\t1915\t250\n",
    "Who Pays?\t1915\t360\n",
    "A Daughter of the Gods\t1916\t180\n",
    "Berg-Ejvind och hans hustru\t1918\t136\n",
    "La España trágica o Tierra de sangre\t1918\t240\n",
    "Vendémiaire\t1918\t148\n",
    "Ingmarssönerna\t1919\t207\n",
    "Time taken: 0.231 seconds, Fetched: 10 row(s)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, under the hood, Hive converts the HiveQL query to a series of map reduce jobs\n",
    "\n",
    "The plan of the conversion can be viewed by prefixing the statement with \"explain\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mysql\n",
    "explain\n",
    "SELECT originaltitle, startyear, runtimeminutes\n",
    "  FROM imdb.title_basics\n",
    "  WHERE startyear <= 1930\n",
    "  AND runtimeminutes > 100\n",
    "  AND genres LIKE 'Drama'\n",
    "  LIMIT 10;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mysql\n",
    "OK\n",
    "Plan not optimized by CBO.\n",
    "\n",
    "Stage-0\n",
    "   Fetch Operator\n",
    "      limit:10\n",
    "      Limit [LIM_3]\n",
    "         Number of rows:10\n",
    "         Select Operator [SEL_2]\n",
    "            outputColumnNames:[\"_col0\",\"_col1\",\"_col2\"]\n",
    "            Filter Operator [FIL_5]\n",
    "               predicate:((startyear <= 1930) and (runtimeminutes > 100) and (genres like 'Drama')) (type: boolean)\n",
    "               TableScan [TS_0]\n",
    "                  alias:title_basics\n",
    "\n",
    "Time taken: 0.091 seconds, Fetched: 14 row(s)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hive is suitable for simpler queries. However as the queries get more complex and need a clearer definition of the dataflow, we should revert to a tool such as Pig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can exit the hive shell with \"quit;\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A HIVE EXERCISE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous sqoop example, you were required to import the tables from the \"world\" database into hive\n",
    "\n",
    "Now first please create a \"world\" database and copy the three tables into that database in hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, in the last example we had provided \"use imdb;\" as the namespace\n",
    "\n",
    "In order to refer to tables not attached to a custom database that we created yet, \"use default;\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can refer to those databases as default.DBNAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mysql\n",
    "hive> show tables in world;\n",
    "OK\n",
    "city\n",
    "country\n",
    "countrylanguage\n",
    "Time taken: 0.638 seconds, Fetched: 3 row(s)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order get information about tables, you can run:\n",
    "\n",
    "```mysql\n",
    "show create table world.city;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mysql\n",
    "hive> show create table world.city;\n",
    "OK\n",
    "CREATE TABLE `world.city`(\n",
    "  `id` int, \n",
    "  `name` string, \n",
    "  `countrycode` string, \n",
    "  `district` string, \n",
    "  `population` int)\n",
    "ROW FORMAT SERDE \n",
    "  'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' \n",
    "STORED AS INPUTFORMAT \n",
    "  'org.apache.hadoop.mapred.TextInputFormat' \n",
    "OUTPUTFORMAT \n",
    "  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
    "LOCATION\n",
    "  'hdfs://sandbox-hdp.hortonworks.com:8020/apps/hive/warehouse/world.db/city'\n",
    "TBLPROPERTIES (\n",
    "  'COLUMN_STATS_ACCURATE'='{\\\"BASIC_STATS\\\":\\\"true\\\"}', \n",
    "  'numFiles'='1', \n",
    "  'numRows'='4079', \n",
    "  'rawDataSize'='140392', \n",
    "  'totalSize'='144471', \n",
    "  'transient_lastDdlTime'='1515066026')\n",
    "Time taken: 0.304 seconds, Fetched: 21 row(s)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output will return information on the schema (column names and types), file type and size information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can repeat it for other tables in the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mysql\n",
    "hive> show create table world.country;\n",
    "OK\n",
    "CREATE TABLE `world.country`(\n",
    "  `code` string, \n",
    "  `name` string, \n",
    "  `continent` string, \n",
    "  `region` string, \n",
    "  `surfacearea` double, \n",
    "  `indepyear` int, \n",
    "  `population` int, \n",
    "  `lifeexpectancy` double, \n",
    "  `gnp` double, \n",
    "  `gnpold` double, \n",
    "  `localname` string, \n",
    "  `governmentform` string, \n",
    "  `headofstate` string, \n",
    "  `capital` int, \n",
    "  `code2` string)\n",
    "ROW FORMAT SERDE \n",
    "  'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' \n",
    "STORED AS INPUTFORMAT \n",
    "  'org.apache.hadoop.mapred.TextInputFormat' \n",
    "OUTPUTFORMAT \n",
    "  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
    "LOCATION\n",
    "  'hdfs://sandbox-hdp.hortonworks.com:8020/apps/hive/warehouse/world.db/country'\n",
    "TBLPROPERTIES (\n",
    "  'COLUMN_STATS_ACCURATE'='{\\\"BASIC_STATS\\\":\\\"true\\\"}', \n",
    "  'numFiles'='1', \n",
    "  'numRows'='239', \n",
    "  'rawDataSize'='30979', \n",
    "  'totalSize'='31218', \n",
    "  'transient_lastDdlTime'='1515066029')\n",
    "Time taken: 0.334 seconds, Fetched: 31 row(s)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "hive> show create table world.countrylanguage;\n",
    "OK\n",
    "CREATE TABLE `world.countrylanguage`(\n",
    "  `countrycode` string, \n",
    "  `language` string, \n",
    "  `isofficial` boolean, \n",
    "  `percentage` double)\n",
    "ROW FORMAT SERDE \n",
    "  'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' \n",
    "STORED AS INPUTFORMAT \n",
    "  'org.apache.hadoop.mapred.TextInputFormat' \n",
    "OUTPUTFORMAT\n",
    "  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
    "LOCATION\n",
    "  'hdfs://sandbox-hdp.hortonworks.com:8020/apps/hive/warehouse/world.db/countrylanguage'\n",
    "TBLPROPERTIES (\n",
    "  'COLUMN_STATS_ACCURATE'='{\\\"BASIC_STATS\\\":\\\"true\\\"}', \n",
    "  'numFiles'='1', \n",
    "  'numRows'='984', \n",
    "  'rawDataSize'='20965', \n",
    "  'totalSize'='21949', \n",
    "  'transient_lastDdlTime'='1515066048')\n",
    "Time taken: 0.305 seconds, Fetched: 20 row(s)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the next task is:\n",
    "\n",
    "- get the names (from country table) of the countries, official languages (from country languages) of which include english "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mysql\n",
    "OK\n",
    "American Samoa\n",
    "Anguilla\n",
    "Antigua and Barbuda\n",
    "Australia\n",
    "Barbados\n",
    "Belize\n",
    "Bermuda\n",
    "United Kingdom\n",
    "Virgin Islands, British\n",
    "Cayman Islands\n",
    "South Africa\n",
    "Falkland Islands\n",
    "Gibraltar\n",
    "Guam\n",
    "Hong Kong\n",
    "Ireland\n",
    "Christmas Island\n",
    "Canada\n",
    "Cocos (Keeling) Islands\n",
    "Lesotho\n",
    "Malta\n",
    "Marshall Islands\n",
    "Montserrat\n",
    "Nauru\n",
    "Niue\n",
    "Norfolk Island\n",
    "Palau\n",
    "Northern Mariana Islands\n",
    "Saint Helena\n",
    "Saint Kitts and Nevis\n",
    "Saint Lucia\n",
    "Saint Vincent and the Grenadines\n",
    "Samoa\n",
    "Seychelles\n",
    "Tokelau\n",
    "Tonga\n",
    "Turks and Caicos Islands\n",
    "Tuvalu\n",
    "New Zealand\n",
    "Vanuatu\n",
    "United States\n",
    "Virgin Islands, U.S.\n",
    "Zimbabwe\n",
    "United States Minor Outlying Islands\n",
    "Time taken: 11.424 seconds, Fetched: 44 row(s)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PIG TUTORIAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pig transforms the declarative nature of Hive into a procedural one, so that dataflow steps are more easily defined "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this examples, we will use HCatalog to connect Pig to Hive databases. HCatalog is a table and storage management layer for Hadoop that enables users with different data processing tools — Pig, MapReduce — to more easily read and write data on the grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enable pig with HCatalog:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "pig -useHCatalog\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will write down the steps defining the workflow and the plan will be executed when we enter the DUMP command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a simple example, let's load city table from world database in Hive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Pig\n",
    "city = LOAD 'world.city' USING org.apache.hive.hcatalog.pig.HCatLoader();\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's select the cities where the countrycode is TUR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Pig\n",
    "cityturkey = filter city by countrycode == 'TUR';\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's select the cities with a population larger than 1 million"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Pig\n",
    "largecitytur = filter cityturkey by population > 1000000;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's execute the plan:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Pig\n",
    "DUMP largecitytur;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Pig\n",
    "(3357,Istanbul,TUR,Istanbul,8787958)\n",
    "(3358,Ankara,TUR,Ankara,3038159)\n",
    "(3359,Izmir,TUR,Izmir,2130359)\n",
    "(3360,Adana,TUR,Adana,1131198)\n",
    "(3361,Bursa,TUR,Bursa,1095842)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's import other tables in the world database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Pig\n",
    "country = LOAD 'world.country' USING org.apache.hive.hcatalog.pig.HCatLoader();\n",
    "\n",
    "\n",
    "lang = LOAD 'world.countrylanguage' USING org.apache.hive.hcatalog.pig.HCatLoader();\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define our previous example in Hive as a Pig dataflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, filter the lang table for countries, official languages of which include English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Pig\n",
    "codeen = filter lang by language == 'English' and isofficial == true;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we join the filtered countrylanguage and country tables on the coeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Pig\n",
    "joinen = JOIN country by code, codeen by countrycode;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And I select only the name field to be dumped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Pig\n",
    "names = foreach joinen generate name;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can execute the plan to dump the names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Pig\n",
    "DUMP names;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Pig\n",
    "(Anguilla)\n",
    "(American Samoa)\n",
    "(Antigua and Barbuda)\n",
    "(Australia)\n",
    "(Belize)\n",
    "(Bermuda)\n",
    "(Barbados)\n",
    "(Canada)\n",
    "(Cocos (Keeling) Islands)\n",
    "(Christmas Island)\n",
    "(Cayman Islands)\n",
    "(Falkland Islands)\n",
    "(United Kingdom)\n",
    "(Gibraltar)\n",
    "(Guam)\n",
    "(Hong Kong)\n",
    "(Ireland)\n",
    "(Saint Kitts and Nevis)\n",
    "(Saint Lucia)\n",
    "(Lesotho)\n",
    "(Marshall Islands)\n",
    "(Malta)\n",
    "(Northern Mariana Islands)\n",
    "(Montserrat)\n",
    "(Norfolk Island)\n",
    "(Niue)\n",
    "(Nauru)\n",
    "(New Zealand)\n",
    "(Palau)\n",
    "(Saint Helena)\n",
    "(Seychelles)\n",
    "(Turks and Caicos Islands)\n",
    "(Tokelau)\n",
    "(Tonga)\n",
    "(Tuvalu)\n",
    "(United States Minor Outlying Islands)\n",
    "(United States)\n",
    "(Saint Vincent and the Grenadines)\n",
    "(Virgin Islands, British)\n",
    "(Virgin Islands, U.S.)\n",
    "(Vanuatu)\n",
    "(Samoa)\n",
    "(South Africa)\n",
    "(Zimbabwe)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PIG EXERCISE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now as an exercise, we will compare the lifeexpentancy of the whole sample and the life expectancy of the countries with English as official language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a feel of calculating averages in Pig, below is the solution for the first part:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Pig\n",
    "-- filter for null values\n",
    "countrynotnull = filter country by lifeexpectancy is not null;\n",
    "\n",
    "-- get lifeexpectancy column\n",
    "\n",
    "lifeall = foreach countrynotnull generate lifeexpectancy;\n",
    "\n",
    "-- combine values into a single group \n",
    "lifeallg = group lifeall all;\n",
    "\n",
    "-- calculate the average\n",
    "avgall = foreach lifeallg generate AVG(lifeall);\n",
    "-- execute\n",
    "DUMP avgall;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is:\n",
    "```Pig\n",
    "(66.486036036036)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now play with code in previous example to get the lifeexpectancy values of English speaking countries (Note that we had extracted the names column. Just change the column)\n",
    "\n",
    "And apply the steps above (you will have the life expectancies of anglophone countries instead of all countries, rest is the same)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that null elimination step is not necessary, the result is the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result should be:\n",
    "```Pig\n",
    "(71.5027027027027)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
